{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following notebook shows the implementation of LinearSVC a part of scikit learns SVM package on the csv file that we created on the Resume Text \n",
    "\n",
    "## Background Details\n",
    "### Working with resume data stored in .csv file job_desc\n",
    "<ul>\n",
    "    <li>reading the given data from csv file</li>\n",
    "    <li>lemmatization and transformation of data</li>\n",
    "    <li>splitting and vectorizing data</li>\n",
    "</ul>\n",
    "\n",
    "### Classification : LinearSVC\n",
    "<ul>\n",
    "    <li>Building a LinearSVC model</li>\n",
    "    <li>Comparing LinearSVC with logistic regression on different metrics</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing important packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Data Wrangling and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Importing from the natural languange toolkit and the scikit lern Library\n",
    "import re   #Importing the regular expression from the regex package\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data from the csv file\n",
    "\n",
    "job_desc = pd.read_csv('C:/Users/User/Resume_Data_Science/job_desc.csv')\n",
    "\n",
    "#Checking out a sample format of the loaded data\n",
    "job_desc.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Transformation of the given data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the lemma and stopwords along with defining the instances for both of them\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stp_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "#Lemmatizing and removing the stopwords\n",
    "#Creating a separate column\n",
    "job_desc['clean'] = job_desc['description'].apply(lambda x: \" \".join([lemmatizer.lemmatize(i) for i in x.split() if i not in words]).lower())\n",
    "\n",
    "\n",
    "#Removing the confusing titles i.e both the 'Data Scientist' and 'Data Analyst' from the title\n",
    "job_desc['title_clean'] = job_desc['title'].map(lambda x: 1 if 'Data Scientist' in x and 'Data Analyst' in x else 0)\n",
    "job_desc.drop(job_desc[job_desc.title_clean == 1].index, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Defining - \n",
    "            0 : 'Data Analyst'\n",
    "            1 : 'Data Scientist'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "job_desc['title_clean'] = job_desc['title'].map(lambda x: 1 if 'Data Scientist' in x else 0)\n",
    "\n",
    "# print random sample to check the format\n",
    "job_desc.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Vectorizing the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all the required preprocessing and the fitting modules from the sklearn package\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer  #used for feature_extraction\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB       #used for Naive_Bayes\n",
    "from sklearn.linear_model import SGDClassifier                               #used for Stochastic Gradient Descent\n",
    "from sklearn.model_selection import LogisticRegression                       #used for Logistic Regression\n",
    "from sklearn.svm import SVC, LinearSVC                                       #used for Support Vector Machine\n",
    "from sklearn.pipeline import Pipeline                                        #used to create a pipeline of methodologies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Building a particular Pipeline\n",
    "text_clf = Pipeline([('vect', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 3))),\n",
    "                     ('clf', LinearSVC())\n",
    "                    ])\n",
    "\n",
    "#Splitting into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(job_desc['clean'], job_desc.title_clean, test_size = 0.2, random_state = 1)\n",
    "\n",
    "\n",
    "#Fitting the data\n",
    "text_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the LinearSVC on the job_desc with testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction using the training dataset\n",
    "y_pred_classify = text_clf.predict(x_test)\n",
    "\n",
    "#Print the classification report\n",
    "#importing metrics from scikit learn\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test,y_pred_classify))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing out the Confusion Matrix\n",
    "metrics.confusion_matrix(y_test,y_pred_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the important feature names for SVM fitting of datasets\n",
    "x_train_tokens = text_clf.named_steps['vect'].get_features_names()\n",
    "\n",
    "#Finding the length of the train_tokens\n",
    "len(x_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a dictionary to store feature and related importance of SVM\n",
    "feature = {}                               #Empty Dictionary\n",
    "\n",
    "#Defining a function to plot important features\n",
    "def plot_coefficients_svm(classifier, feature_names, top_features = 30):\n",
    "    #Getting the coefficients and store it to coef\n",
    "    coef = classifier.coef_.ravel()\n",
    "    \n",
    "    #Returning the index value of top positive and negative parameters\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    \n",
    "    \n",
    "    # plot the graph\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    colors = ['red' if c<0 else 'blue' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2*top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 1+2*top_features), feature_names[top_coefficients],rotation=60, ha='right')\n",
    "    plt.show()                           #Displaying the plot\n",
    "    \n",
    "    \n",
    "    #store highest correlated parameters to feature.\n",
    "    for i in top_coefficients[:30]:\n",
    "        feature[feature_names[i]] = -coef[i]\n",
    "        \n",
    "\n",
    "\n",
    "cv = text_clf.named_steps['vect']\n",
    "svm = text_clf.named_steps['clf']\n",
    "\n",
    "#Calling the function with the values\n",
    "plot_coefficients_svm(svm, cv.get_feature_names())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a cloudword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 600)\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
